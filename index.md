---
layout: home
title: Summary
---

I’m a PhD student at University of Amsterdam and Samsung AI supervised by Dmitry Vetrov and Max Welling. My research is focused on Bayesian Deep Learning. 
Also, I'm a teacher assistant at several Machine Learning courses of our [scientific group](http://bayesgroup.ru/teaching/).
<br />
<br />
My CV is available [here](https://docs.google.com/document/d/1s7wPBTrbM4LNM9elzAjepc5RLGLobmzAiVVUy1kxDTU/edit?usp=sharing).
 
## News 
- [The Deep Weight Prior](https://arxiv.org/abs/1810.06943) and [Variance Networks](https://arxiv.org/abs/1803.03764) got accepted to ICLR'19!
- We released [preprint](https://arxiv.org/abs/1810.06943) on The Deep Weight Prior.
- Two papers [Uncertainty Estimation via Stochastic Batch Normalization](https://openreview.net/forum?id=r1yXEdkvz), [Bayesian Incremental Learning for Deep Neural Networks](https://openreview.net/forum?id=ByZzFPJDG) have been accepted to ICLR Workshop 2018!
- We released [preprint](https://arxiv.org/abs/1803.03764) on Variance Networks: When Expectation Does Not Meet Your Expectations.
- Our StructuredBP via Log-Normal Multiplicative Noise [paper](https://arxiv.org/abs/1705.07283) was accepted to NIPS’17.
- I joined [AMLab at University of Amsterdam](http://amlab.science.uva.nl/people/) and Yandex Research ([home page](https://research.yandex.com/lib/people/609475)). 
- We released [preprint](https://arxiv.org/abs/1705.07283) on Structured Bayesian Pruning via Log-Normal Multiplicative Noise.
- Our Variational Dropout Sparsifies DNNs [paper](https://arxiv.org/abs/1701.05369) was accepted to ICML’17 ([github](https://github.com/ars-ashuha/variational-dropout-sparsifies-dnn)).
- I joined [HSE Bayesian Deep Learning Lab](https://cs.hse.ru/en/big-data/bayeslab), here is my hse [home page](https://www.hse.ru/en/org/persons/204848606).
- We released [preprint](https://arxiv.org/abs/1701.05369) on Variational Dropout Sparsifies DNNs.
