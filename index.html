<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Senya is Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167092781-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-167092781-1');
  </script>

  <script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>


  <title>Senya Ashukha</title>
  
  <meta name="author" content="Senya Ashukha">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Arsenii Ashukha</name>
              </p>
              <p> AI Researcher <a href="https://research.samsung.com/aicenter_moscow">@Samsung AI</a> <a href="https://twitter.com/bayesgroup">@bayesgroup</a> making computers smarter üëæü¶æ  
              </p>
              <p align=center>
                <a href="mailto:ars.ashuha@gmail.com" target="_blank">Email</a> &nbsp/&nbsp
                <a href="https://senya-ashukha.github.io/arsenii-ashukha-cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=IU-kuP8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/senya-ashukha">GitHub</a> &nbsp/&nbsp
                <a href="https://twitter.com/senya_ashuha">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/images/senya-ashukha.jpg"><img style="border-radius:50%;width:100%;max-width:100%" alt="profile photo" src="images/senya-ashukha.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My works are focused on understanding and applications of ensemble techniques and variational inference in deep neural networks. 
                Representative papers are marked by an orange.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/svdo_icml17/svdo_prev.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle" bgcolor="#ffffff">
              <a href="http://proceedings.mlr.press/v70/molchanov17a.html">
                <papertitle>üçäVariational Dropout Sparsifies Deep Neural Networks</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>ICML</em>, 2017  
              <br>
               <a href="javascript:toggle_vis('contact2')"  style="color:gray"><b>retrospective‚è≥</b></a> /
                <a href="https://iclr.cc/virtual_2020/poster_BJxI5gHKDr.html">talk (15 mins)</a> /  
                <a href="https://arxiv.org/abs/1701.05369">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/svdo_icml17/paper.txt" target="_blank">bibtex</a> / 
                code (<a href="https://github.com/bayesgroup/variational-dropout-sparsifies-dnn">theano</a>,
                <a href="https://github.com/google-research/google-research/tree/master/state_of_sparsity/layers/variational_dropout" target="_blank">tf by GoogleAI</a>,
                <a href="https://colab.research.google.com/github/bayesgroup/deepbayes-2019/blob/master/seminars/day6/SparseVD-solution.ipynb" target="_blank">colab pytorch</a>)
                  <div id="contact2" style="display: none;"> 
                    <br>
                    <br>
                    <b>Retrospective</b>: 
                    <i>i)</i> SparsesVD indeed works, and I even heard about several practical use cases in leading IT companies! However, careful usage of pruning often <a href="https://arxiv.org/abs/1902.09574">can produce</a> better results. 
                    <i>ii)</i> Training of deep models with noise is known to be hard and unstable. That is less the case with SparseVD. All variances, by a happy coincidence, have been initialized with small values and did not change much during training. Using small constant variances does not hurt the performance, so SparseVD might be considered as a fancy regulariser with (almost) no noise. 
                    <i>iii)</i> The sparse solution is just a local optimum, as better values of ELBO <a href="https://openreview.net/forum?id=B1GAUs0cKQ">can be achieved</a> with a <ins>less</ins> flexible variational posterior q(w_ij)=N(w_ij | 0, œÉ_ij).
                   </p>
                  </div>
              <p></p>
              <p>
              Variational dropout secretly trains highly sparsified deep neural networks, while a pattern of sparsity is learned jointly with weights during training.</p>
            </td>
          </tr>
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/pitfalls_unc_ens_iclr20/pic.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle" bgcolor="#ffffff">
              <a href="https://openreview.net/forum?id=BJxI5gHKDr">
                <papertitle>üçäPitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning </papertitle>
              </a>
              <br>
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.ru/citations?user=5LXTi40AAAAJ&hl=en">Alexander Lyzhov*</a>,
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>ICLR</em>, 2020  
              <br>
              <a href="https://senya-ashukha.github.io/pitfalls-uncertainty&ensembling">blog post</a> / 
                <a href="https://iclr.cc/virtual_2020/poster_BJxI5gHKDr.html">poster video (5mins)</a> / 
                <a href="https://github.com/bayesgroup/pytorch-ensembles">code</a> / 
                <a href="https://arxiv.org/abs/2002.06470">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/pitfalls_unc_ens_iclr20/paper.txt" target="_blank">bibtex</a>
              <p></p>
              <p>
            The work shows that an ensemble of independently trained networks performs well ahead of competitors! Also, we point out pitfalls of UE metrics and point out that <i>test-time augmentation</i>  is a simple technique that allows to improve ensembles for free.</p>  
            </td>
          </tr> 
        </table>

        
<!--       <div align="right">
      <a href="javascript:toggle_vis('contact3')" style="color:gray;"><b>show more ...</b></a>
      </div> -->
        

        <div id="contact3" style="display: none;"> 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:10px;width:25%;vertical-align:middle">
                <img src='projects/gps_uai20/gps.png' width="170px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="http://proceedings.mlr.press/v124/lyzhov20a.html">
                <papertitle>Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <a href="https://scholar.google.ru/citations?user=5LXTi40AAAAJ&hl=en">Alexander Lyzhov*</a>,
              <a href="">Yuliya Molchanova*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>UAI</em>, 2020  
              <br>
                <a href="https://github.com/bayesgroup/gps-augment">code</a> / 
                <a href="https://arxiv.org/abs/2002.09103">arXiv</a> / 
               <a href="https://senya-ashukha.github.io/projects/gps_uai20/gps_uai20_slides.pdf" target="_blank">slides</a> /
                <a href="https://senya-ashukha.github.io/projects/gps_uai20/paper.txt" target="_blank">bibtex</a>
              <p></p>
              <p>
            We introduce greedy policy search (GPS), a simple but high-performing method for learning a policy of test-time augmentation.   
            </td>
          </tr> 


          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/dwp_iclr19/dwp.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=ByGuynAct7">
                <papertitle>The Deep Weight Prior</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=XriU_R8AAAAJ&hl=en">Andrei Atanov*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=q69zIO0AAAAJ&hl=en">Kirill Struminsky</a>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>,
              <a href="https://staff.fnwi.uva.nl/m.welling/">Max Welling</a>
              <br>
                <em>ICLR</em>, 2019  
              <br>
                <a href="https://github.com/bayesgroup/deep-weight-prior">code</a> /
                <a href="https://arxiv.org/abs/1810.06943">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/dwp_iclr19/paper.txt" target="_blank">bibtex</a> 
              <p></p>
              <p>
              The <i>deep weight prior</i> is the generative model for kernels of convolutional neural networks, that acts as a prior distribution while training on new datasets.</p>
            </td>
          </tr> 

          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/vn-iclr19/vn.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=B1GAUs0cKQ">
                <papertitle>Variance Networks: When Expectation Does Not Meet Your Expectations</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.ru/citations?user=eOttYWgAAAAJ&hl=en">Kirill Neklyudov*</a>,
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>ICLR</em>, 2019  
              <br>
                <a href="https://github.com/da-molchanov/variance-networks">code</a> /
                <a href="https://arxiv.org/abs/1803.03764">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/vn-iclr19/paper.txt" target="_blank">bibtex</a> 
              <p></p>
              <p>
              It is possible to learn a zero-centered Gaussian distribution over the weights of a neural network by learning only variances, and it works surprisingly well.</p>
            </td>
          </tr> 

          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/sbn_iclrw18/sbn.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=r1yXEdkvz">
                <papertitle>Uncertainty Estimation via Stochastic Batch Normalization</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=XriU_R8AAAAJ&hl=en">Andrei Atanov</a>,
              <strong>Arsenii Ashukha</strong>,
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov</a>,
              <a href="https://scholar.google.ru/citations?user=eOttYWgAAAAJ&hl=en">Kirill Neklyudov</a>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>ICLR Workshop</em>, 2018  
              <br>
                <a href="https://github.com/AndrewAtanov/stochastic-batch-normalization">code</a> /
                <a href="https://arxiv.org/abs/1802.04893">arXiv</a>
              <p></p>
              <p>Inference-time stochastic batch normalization improves the performance of uncertainty estimation of ensembles.</p>
            </td>
          </tr> 

            <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/sbp_neurips17/sbp.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="http://papers.nips.cc/paper/7254-structured-bayesian-pruning-via-log-normal-multiplicative-noise">
                <papertitle>Structured Bayesian Pruning via Log-Normal Multiplicative Noise</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.ru/citations?user=eOttYWgAAAAJ&hl=en">Kirill Neklyudov</a>,
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov</a>,
              <strong>Arsenii Ashukha</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>NeurIPS</em>, 2017  
              <br>
               <a href="https://github.com/necludov/group-sparsity-sbp">code</a> / 
                <a href="https://arxiv.org/abs/1705.07283">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/sbp_neurips17/paper.txt" target="_blank">bibtex</a> / 
                <a href="https://bayesgroup.github.io/pdf/sbp-poster.pdf">poster</a> 
              <p></p>
              <p>
              The model allows to sparsify a DNN with an arbitrary pattern of spasticity e.g., neurons or convolutional filters. 
              
            </td>
          </tr> 
          </table>
        </div>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <heading>Code</heading>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p align="left">
              Check out very short and simple and fan to make implementations of ML algorithms:
                <ul>
                <li><a href="https://github.com/senya-ashukha/simple-gradient-boosting">Gradient Boosting</a></li>
                <li><a href="https://github.com/senya-ashukha/real-nvp-pytorch">Real NVP</a></li>
                <li><a href="https://github.com/senya-ashukha/quantile-regression-dqn-pytorch">Quantile Regression DQN (Distributional RL)</a></li>
              </ul>
              <br>
              Also, chek out more solid implementations (at least they can do ImageNet):
              <ul>
                <li><a href="https://github.com/AndrewAtanov/simclr-pytorch">Multi-gpu SimCLRv1</a></li>
                <li><a href="https://github.com/bayesgroup/pytorch-ensembles">Ensembles (Deep ensembles, Snapshot ensembles, cSGLD, FGE, etc.)</li>
              <ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          
            <tr>
              <tr>
                <heading>Talks</heading>
            <td style="padding:20px;width:100%;vertical-align:middle">
              The State of Ensembling (and Uncertainty), Scientific seminar of <a href="https://twitter.com/bayesgroup">@bayesgroup</a>, 2019
              <p></p>
              <div align="center">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/lNIoJmi2tO8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </td>
          </tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              
              <p>
                Normalizing flows, Deep|Bayes summer school, 2019
                <p></p>
                <div align="center">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/v4gp1dMvWJo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              How to generate Ensembles, <a href="https://twitter.com/AndreyMalinin/status/1337116698428776449">Uncertainty estimation mini-course</a>, 2020
  
              <p style="color:gray;">Grammatical errors are hard to control sometimes, and it should be 0.5(f1(x) + f2(x))</p>
              <p></p>
              <div align="center">
              <iframe width="560" height="315" src="https://www.youtube.com/embed/bj933t6rqFw?start=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </td>
          </tr>

            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              Variational dropout (in russian), <a href="https://datafest.ru/">Data fest</a>, 2019
              <p></p>
              <div align="center">
              <iframe width="560" height="315" src="https://www.youtube.com/embed/4je6xYm-b8Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p align="right">
                The webpage template was borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">Jon Barron</a>.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
